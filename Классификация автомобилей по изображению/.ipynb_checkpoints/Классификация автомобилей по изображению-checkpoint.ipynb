{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация изображений\n",
    "\n",
    "### Основная идея этого решения: взять предобученую на ImageNet сеть efficientnet и дообучить под нашу задачу. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 15 21:48:28 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.zip', 'sample-submission.csv', 'test.zip', 'train.csv']\n",
      "Python       : 3.7.9 | packaged by conda-forge | (default, Feb 13 2021, 20:03:11) \n",
      "Numpy        : 1.19.5\n",
      "Tensorflow   : 2.4.1\n",
      "Keras        : 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import zipfile\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.layers import *\n",
    "from IPython.display import FileLink\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.callbacks as C\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "\n",
    "\n",
    "import PIL\n",
    "from PIL import ImageOps, ImageFilter\n",
    "#увеличим дефолтный размер графиков\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "#графики в svg выглядят более четкими\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "%matplotlib inline\n",
    "\n",
    "print(os.listdir(\"../input\"))\n",
    "print('Python       :', sys.version.split('\\n')[0])\n",
    "print('Numpy        :', np.__version__)\n",
    "print('Tensorflow   :', tf.__version__)\n",
    "print('Keras        :', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/mjkvaak/ImageDataAugmentor\n",
      "  Cloning https://github.com/mjkvaak/ImageDataAugmentor to /tmp/pip-req-build-ez0nwuvy\n",
      "  Running command git clone -q https://github.com/mjkvaak/ImageDataAugmentor /tmp/pip-req-build-ez0nwuvy\n",
      "Requirement already satisfied: opencv-python>=4.2 in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (4.5.1.48)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (3.4.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (7.2.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (1.5.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (1.1.5)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from opencv-python>=4.2->ImageDataAugmentor==0.0.0) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ImageDataAugmentor==0.0.0) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ImageDataAugmentor==0.0.0) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ImageDataAugmentor==0.0.0) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ImageDataAugmentor==0.0.0) (2.8.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->ImageDataAugmentor==0.0.0) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->ImageDataAugmentor==0.0.0) (2021.1)\n",
      "Building wheels for collected packages: ImageDataAugmentor\n",
      "  Building wheel for ImageDataAugmentor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ImageDataAugmentor: filename=ImageDataAugmentor-0.0.0-py3-none-any.whl size=29531 sha256=a7540e60f41b2417156bc8052594e7bc77ea1bf69f72615d9d6fe5d8d3fb5097\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-j48lrlem/wheels/c9/bd/73/9cfa59d2393dae55bbcc30f5aa901f55fe531c66efebbc8fc3\n",
      "Successfully built ImageDataAugmentor\n",
      "Installing collected packages: ImageDataAugmentor\n",
      "Successfully installed ImageDataAugmentor-0.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/mjkvaak/ImageDataAugmentor\n",
    "!pip install -q efficientnet\n",
    "import efficientnet.tfkeras as efn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основные настройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В setup выносим основные настройки: так удобнее их перебирать в дальнейшем.\n",
    "\n",
    "EPOCHS               = 15  # эпох на обучение\n",
    "BATCH_SIZE           = 32 # уменьшаем batch если сеть большая, иначе не влезет в память на GPU\n",
    "LR                   = 1e-3\n",
    "VAL_SPLIT            = 0.15 # сколько данных выделяем на тест = 15%\n",
    "\n",
    "CLASS_NUM            = 10  # количество классов в нашей задаче\n",
    "IMG_SIZE             = 224 # какого размера подаем изображения в сеть\n",
    "IMG_CHANNELS         = 3   # у RGB 3 канала\n",
    "input_shape          = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n",
    "\n",
    "DATA_PATH = '../input/'\n",
    "PATH = \"../working/car/\" # рабочая директория"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Устаналиваем конкретное значение random seed для воспроизводимости\n",
    "os.makedirs(PATH,exist_ok=False)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)  \n",
    "PYTHONHASHSEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA / Анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100155.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100306.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100379.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100380.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100389.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id  Category\n",
       "0  100155.jpg         0\n",
       "1  100306.jpg         0\n",
       "2  100379.jpg         0\n",
       "3  100380.jpg         0\n",
       "4  100389.jpg         0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(DATA_PATH+\"train.csv\")\n",
    "sample_submission = pd.read_csv(DATA_PATH+\"sample-submission.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15561 entries, 0 to 15560\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Id        15561 non-null  object\n",
      " 1   Category  15561 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 243.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1971\n",
       "8    1765\n",
       "6    1733\n",
       "5    1631\n",
       "0    1613\n",
       "3    1528\n",
       "2    1458\n",
       "4    1400\n",
       "9    1255\n",
       "7    1207\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Category.value_counts()\n",
    "# распределение классов достаточно равномерное - это хорошо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распаковываем картинки\n",
      "['test_upload', 'train']\n"
     ]
    }
   ],
   "source": [
    "print('Распаковываем картинки')\n",
    "# Will unzip the files so that you can see them..\n",
    "for data_zip in ['train.zip', 'test.zip']:\n",
    "    with zipfile.ZipFile(\"../input/\"+data_zip,\"r\") as z:\n",
    "        z.extractall(PATH)\n",
    "        \n",
    "print(os.listdir(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аугментация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ImageDataAugmentor.image_data_augmentor import *\n",
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENTATIONS = albumentations.Compose([\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.Rotate(limit=30, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.5),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.CenterCrop(height=224, width=200),\n",
    "        albumentations.CenterCrop(height=200, width=224),\n",
    "    ],p=0.5),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n",
    "        albumentations.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1)\n",
    "    ],p=0.5),\n",
    "    albumentations.GaussianBlur(p=0.05),\n",
    "    albumentations.HueSaturationValue(p=0.5),\n",
    "    albumentations.RGBShift(p=0.5),\n",
    "    albumentations.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n",
    "    albumentations.Resize(IMG_SIZE, IMG_SIZE)\n",
    "])\n",
    "\n",
    "train_datagen = ImageDataAugmentor(\n",
    "        rescale=1./255,\n",
    "        augment = AUGMENTATIONS,\n",
    "        validation_split=VAL_SPLIT,\n",
    "        )\n",
    "        \n",
    "test_datagen = ImageDataAugmentor(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ImageDataAugmentor/image_data_augmentor.py:332: UserWarning: Passing `seed` in `.flow_from_directory` has been been removed: pass  `seed` as parameter in `ImageDataAugmentor(..., seed=...)` instead\n",
      "  warnings.warn('Passing `seed` in `.flow_from_directory` has been been removed: pass  `seed` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13232 images belonging to 10 classes.\n",
      "Found 2329 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Завернем наши данные в генератор:\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',      # директория где расположены папки с картинками \n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='training') # set as training data\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем предобученную сеть efficientnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b7_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "258441216/258434480 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = efn.EfficientNetB7(weights='imagenet', include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заморозим веса efficientnetB7 и обучим пока только \"голову\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=M.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(L.GlobalAveragePooling2D(),) # объединяем все признаки в единый вектор \n",
    "\n",
    "#В качестве эксперимента с архитектурой модели добавим полносвязный слой, \n",
    "#dropout и batch-normalization\n",
    "model.add(L.Dense(256, activation='elu'))\n",
    "model.add(L.BatchNormalization())\n",
    "model.add(L.Dropout(0.25))\n",
    "model.add(L.Dense(CLASS_NUM, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b7 (Functional) (None, 7, 7, 2560)        64097680  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               655616    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 64,756,890\n",
      "Trainable params: 658,698\n",
      "Non-trainable params: 64,098,192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим ModelCheckpoint чтоб сохранять прогресс обучения модели и можно было потом подгрузить и дообучить модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model.hdf5' , monitor = ['val_accuracy'] , verbose = 1  , mode = 'max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.25,\n",
    "                              patience=2,\n",
    "                              min_lr=0.00001,\n",
    "                              verbose=1,\n",
    "                              mode='auto')\n",
    "earlystop = EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, earlystop, reduce_lr]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "413/413 [==============================] - 206s 450ms/step - loss: 2.0143 - accuracy: 0.3557 - val_loss: 1.1416 - val_accuracy: 0.6094\n",
      "\n",
      "Epoch 00001: saving model to best_model.hdf5\n",
      "Epoch 2/15\n",
      "413/413 [==============================] - 178s 430ms/step - loss: 1.4549 - accuracy: 0.4807 - val_loss: 1.0354 - val_accuracy: 0.6311\n",
      "\n",
      "Epoch 00002: saving model to best_model.hdf5\n",
      "Epoch 3/15\n",
      "413/413 [==============================] - 176s 426ms/step - loss: 1.3634 - accuracy: 0.5191 - val_loss: 0.9829 - val_accuracy: 0.6680\n",
      "\n",
      "Epoch 00003: saving model to best_model.hdf5\n",
      "Epoch 4/15\n",
      "413/413 [==============================] - 179s 432ms/step - loss: 1.2868 - accuracy: 0.5449 - val_loss: 0.9282 - val_accuracy: 0.6840\n",
      "\n",
      "Epoch 00004: saving model to best_model.hdf5\n",
      "Epoch 5/15\n",
      "413/413 [==============================] - 177s 429ms/step - loss: 1.2205 - accuracy: 0.5755 - val_loss: 0.9054 - val_accuracy: 0.6827\n",
      "\n",
      "Epoch 00005: saving model to best_model.hdf5\n",
      "Epoch 6/15\n",
      "413/413 [==============================] - 177s 427ms/step - loss: 1.2129 - accuracy: 0.5700 - val_loss: 0.8849 - val_accuracy: 0.6888\n",
      "\n",
      "Epoch 00006: saving model to best_model.hdf5\n",
      "Epoch 7/15\n",
      "413/413 [==============================] - 177s 429ms/step - loss: 1.2075 - accuracy: 0.5755 - val_loss: 0.8859 - val_accuracy: 0.6871\n",
      "\n",
      "Epoch 00007: saving model to best_model.hdf5\n",
      "Epoch 8/15\n",
      "413/413 [==============================] - 177s 428ms/step - loss: 1.1871 - accuracy: 0.5824 - val_loss: 0.8707 - val_accuracy: 0.6927\n",
      "\n",
      "Epoch 00008: saving model to best_model.hdf5\n",
      "Epoch 9/15\n",
      "413/413 [==============================] - 178s 430ms/step - loss: 1.1734 - accuracy: 0.5866 - val_loss: 0.8337 - val_accuracy: 0.7057\n",
      "\n",
      "Epoch 00009: saving model to best_model.hdf5\n",
      "Epoch 10/15\n",
      "413/413 [==============================] - 177s 428ms/step - loss: 1.1516 - accuracy: 0.5901 - val_loss: 0.8707 - val_accuracy: 0.6940\n",
      "\n",
      "Epoch 00010: saving model to best_model.hdf5\n",
      "Epoch 11/15\n",
      "413/413 [==============================] - 177s 429ms/step - loss: 1.1275 - accuracy: 0.6009 - val_loss: 0.8029 - val_accuracy: 0.7174\n",
      "\n",
      "Epoch 00011: saving model to best_model.hdf5\n",
      "Epoch 12/15\n",
      "413/413 [==============================] - 177s 429ms/step - loss: 1.1169 - accuracy: 0.6024 - val_loss: 0.8031 - val_accuracy: 0.7161\n",
      "\n",
      "Epoch 00012: saving model to best_model.hdf5\n",
      "Epoch 13/15\n",
      "413/413 [==============================] - 176s 427ms/step - loss: 1.1135 - accuracy: 0.5983 - val_loss: 0.7839 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00013: saving model to best_model.hdf5\n",
      "Epoch 14/15\n",
      "413/413 [==============================] - 177s 428ms/step - loss: 1.0902 - accuracy: 0.6127 - val_loss: 0.7998 - val_accuracy: 0.7296\n",
      "\n",
      "Epoch 00014: saving model to best_model.hdf5\n",
      "Epoch 15/15\n",
      "413/413 [==============================] - 177s 429ms/step - loss: 1.1034 - accuracy: 0.6123 - val_loss: 0.7904 - val_accuracy: 0.7261\n",
      "\n",
      "Epoch 00015: saving model to best_model.hdf5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "    validation_data = test_generator, \n",
    "    validation_steps = test_generator.samples//test_generator.batch_size,\n",
    "    epochs = EPOCHS,\n",
    "    callbacks = callbacks_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 27s 361ms/step - loss: 0.7876 - accuracy: 0.7295\n",
      "Accuracy: 72.95%\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (res[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним итоговую сеть и подгрузим лучшую итерацию в обучении (best_model)\n",
    "model.save('../working/model_last.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - FineTuning - обучение половины весов EfficientNetb7¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "\n",
    "fine_tune_at = len(base_model.layers)//2\n",
    "#заморозим все веса до середины\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13232 images belonging to 10 classes.\n",
      "Found 2329 images belonging to 10 classes.\n",
      "Epoch 1/10\n",
      "827/827 [==============================] - 273s 304ms/step - loss: 1.3823 - accuracy: 0.5464 - val_loss: 0.3816 - val_accuracy: 0.8612\n",
      "\n",
      "Epoch 00001: saving model to best_model.hdf5\n",
      "Epoch 2/10\n",
      "827/827 [==============================] - 248s 300ms/step - loss: 0.5386 - accuracy: 0.8073 - val_loss: 0.2684 - val_accuracy: 0.9030\n",
      "\n",
      "Epoch 00002: saving model to best_model.hdf5\n",
      "Epoch 3/10\n",
      "827/827 [==============================] - 249s 301ms/step - loss: 0.3717 - accuracy: 0.8701 - val_loss: 0.2368 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00003: saving model to best_model.hdf5\n",
      "Epoch 4/10\n",
      "827/827 [==============================] - 248s 300ms/step - loss: 0.2999 - accuracy: 0.8895 - val_loss: 0.2065 - val_accuracy: 0.9306\n",
      "\n",
      "Epoch 00004: saving model to best_model.hdf5\n",
      "Epoch 5/10\n",
      "827/827 [==============================] - 249s 300ms/step - loss: 0.2466 - accuracy: 0.9164 - val_loss: 0.2036 - val_accuracy: 0.9276\n",
      "\n",
      "Epoch 00005: saving model to best_model.hdf5\n",
      "Epoch 6/10\n",
      "827/827 [==============================] - 248s 299ms/step - loss: 0.2113 - accuracy: 0.9258 - val_loss: 0.1898 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00006: saving model to best_model.hdf5\n",
      "Epoch 7/10\n",
      "827/827 [==============================] - 248s 300ms/step - loss: 0.1945 - accuracy: 0.9318 - val_loss: 0.1926 - val_accuracy: 0.9362\n",
      "\n",
      "Epoch 00007: saving model to best_model.hdf5\n",
      "Epoch 8/10\n",
      "827/827 [==============================] - 249s 301ms/step - loss: 0.1665 - accuracy: 0.9418 - val_loss: 0.1875 - val_accuracy: 0.9409\n",
      "\n",
      "Epoch 00008: saving model to best_model.hdf5\n",
      "Epoch 9/10\n",
      "827/827 [==============================] - 250s 302ms/step - loss: 0.1444 - accuracy: 0.9502 - val_loss: 0.1930 - val_accuracy: 0.9405\n",
      "\n",
      "Epoch 00009: saving model to best_model.hdf5\n",
      "Epoch 10/10\n",
      "827/827 [==============================] - 249s 301ms/step - loss: 0.1394 - accuracy: 0.9512 - val_loss: 0.1860 - val_accuracy: 0.9401\n",
      "\n",
      "Epoch 00010: saving model to best_model.hdf5\n"
     ]
    }
   ],
   "source": [
    "EPOCHS               = 10  # эпох на обучение\n",
    "BATCH_SIZE           = 16 # уменьшаем batch если сеть большая, иначе не влезет в память на GPU\n",
    "LR                   = 1e-4\n",
    "VAL_SPLIT            = 0.2 # сколько данных выделяем на тест = 20%\n",
    "\n",
    "CLASS_NUM            = 10  # количество классов в нашей задаче\n",
    "IMG_SIZE             = 224 # какого размера подаем изображения в сеть\n",
    "IMG_CHANNELS         = 3   # у RGB 3 канала\n",
    "input_shape          = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n",
    "\n",
    "DATA_PATH = '../input/'\n",
    "PATH = \"../working/car/\" # рабочая директория\n",
    "\n",
    "# Устанавливаем конкретное значение random seed для воспроизводимости\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)  \n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "# Завернем наши данные в генератор:\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',      # директория где расположены папки с картинками \n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='training') # set as training data\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model.hdf5' , monitor = ['accuracy'] , verbose = 1  , mode = 'max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.25,\n",
    "                              patience=2,\n",
    "                              min_lr=0.0000001,\n",
    "                              verbose=1,\n",
    "                              mode='auto')\n",
    "earlystop = EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, earlystop, reduce_lr]\n",
    "\n",
    "# Обучаем\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "    validation_data = test_generator, \n",
    "    validation_steps = test_generator.samples//test_generator.batch_size,\n",
    "    epochs = EPOCHS,\n",
    "    callbacks = callbacks_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 28s 192ms/step - loss: 0.1750 - accuracy: 0.9407\n",
      "Accuracy: 94.07%\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (res[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним модель\n",
    "model.save('../working/model_step2.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - FineTuning - разморозка всей сети EfficientNetB7 и дообучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13232 images belonging to 10 classes.\n",
      "Found 2329 images belonging to 10 classes.\n",
      "Epoch 1/10\n",
      "1654/1654 [==============================] - 655s 378ms/step - loss: 0.2166 - accuracy: 0.9268 - val_loss: 0.1570 - val_accuracy: 0.9519\n",
      "\n",
      "Epoch 00001: saving model to best_model.hdf5\n",
      "Epoch 2/10\n",
      "1654/1654 [==============================] - 622s 376ms/step - loss: 0.1570 - accuracy: 0.9468 - val_loss: 0.1402 - val_accuracy: 0.9562\n",
      "\n",
      "Epoch 00002: saving model to best_model.hdf5\n",
      "Epoch 3/10\n",
      "1654/1654 [==============================] - 621s 375ms/step - loss: 0.1446 - accuracy: 0.9497 - val_loss: 0.1398 - val_accuracy: 0.9558\n",
      "\n",
      "Epoch 00003: saving model to best_model.hdf5\n",
      "Epoch 4/10\n",
      "1654/1654 [==============================] - 623s 377ms/step - loss: 0.1357 - accuracy: 0.9543 - val_loss: 0.1490 - val_accuracy: 0.9558\n",
      "\n",
      "Epoch 00004: saving model to best_model.hdf5\n",
      "Epoch 5/10\n",
      "1654/1654 [==============================] - 622s 376ms/step - loss: 0.1136 - accuracy: 0.9597 - val_loss: 0.1495 - val_accuracy: 0.9558\n",
      "\n",
      "Epoch 00005: saving model to best_model.hdf5\n",
      "Epoch 6/10\n",
      "1654/1654 [==============================] - 623s 377ms/step - loss: 0.1198 - accuracy: 0.9561 - val_loss: 0.1483 - val_accuracy: 0.9605\n",
      "\n",
      "Epoch 00006: saving model to best_model.hdf5\n",
      "Epoch 7/10\n",
      "1654/1654 [==============================] - 622s 376ms/step - loss: 0.1089 - accuracy: 0.9646 - val_loss: 0.1341 - val_accuracy: 0.9618\n",
      "\n",
      "Epoch 00007: saving model to best_model.hdf5\n",
      "Epoch 8/10\n",
      "1654/1654 [==============================] - 623s 377ms/step - loss: 0.0986 - accuracy: 0.9671 - val_loss: 0.1470 - val_accuracy: 0.9609\n",
      "\n",
      "Epoch 00008: saving model to best_model.hdf5\n",
      "Epoch 9/10\n",
      "1654/1654 [==============================] - 623s 376ms/step - loss: 0.0877 - accuracy: 0.9694 - val_loss: 0.1542 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00009: saving model to best_model.hdf5\n",
      "Epoch 10/10\n",
      "1654/1654 [==============================] - 622s 376ms/step - loss: 0.0889 - accuracy: 0.9698 - val_loss: 0.1449 - val_accuracy: 0.9592\n",
      "\n",
      "Epoch 00010: saving model to best_model.hdf5\n"
     ]
    }
   ],
   "source": [
    "EPOCHS               = 10  # эпох на обучение\n",
    "BATCH_SIZE           = 8 # уменьшаем batch если сеть большая, иначе не влезет в память на GPU\n",
    "LR                   = 1e-5\n",
    "VAL_SPLIT            = 0.2 # сколько данных выделяем на тест = 20%\n",
    "\n",
    "CLASS_NUM            = 10  # количество классов в нашей задаче\n",
    "IMG_SIZE             = 224 # какого размера подаем изображения в сеть\n",
    "IMG_CHANNELS         = 3   # у RGB 3 канала\n",
    "input_shape          = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n",
    "\n",
    "\n",
    "# Устанавливаем конкретное значение random seed для воспроизводимости\n",
    "\n",
    "# Завернем наши данные в генератор:\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',      # директория где расположены папки с картинками \n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='training') # set as training data\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])\n",
    "\n",
    "# Добавим ModelCheckpoint чтоб сохранять прогресс обучения модели и можно было потом подгрузить и дообучить модель.    \n",
    "checkpoint = ModelCheckpoint('best_model.hdf5' , monitor = ['accuracy'] , verbose = 1  , mode = 'max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.25,\n",
    "                              patience=2,\n",
    "                              min_lr=0.0000001,\n",
    "                              verbose=1,\n",
    "                              mode='auto')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, earlystop]\n",
    "\n",
    "# Обучаем\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "    validation_data = test_generator, \n",
    "    validation_steps = test_generator.samples//test_generator.batch_size,\n",
    "    epochs = EPOCHS,\n",
    "    callbacks = callbacks_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../working/model_step3.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/292 [==============================] - 30s 101ms/step - loss: 0.1523 - accuracy: 0.9579\n",
      "Accuracy: 95.79%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - увеличим размер изображения и понизим уровень аугментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE             = 512\n",
    "BATCH_SIZE           = 2\n",
    "LR                   = 1e-5\n",
    "EPOCHS               = 3  # эпох на обучение\n",
    "\n",
    "input_shape          = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n",
    "\n",
    "AUGMENTATIONS = albumentations.Compose([\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.Rotate(limit=30, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.5)\n",
    "])\n",
    "\n",
    "train_datagen = ImageDataAugmentor(\n",
    "        rescale=1./255,\n",
    "        augment = AUGMENTATIONS,\n",
    "        validation_split=VAL_SPLIT,\n",
    "        )\n",
    "        \n",
    "test_datagen = ImageDataAugmentor(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12452 images belonging to 10 classes.\n",
      "Found 3109 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',      # директория где расположены папки с картинками \n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='training') # set as training data\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим сеть с новым размером входных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = efn.EfficientNetB7(weights='imagenet', include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "6226/6226 [==============================] - 3280s 522ms/step - loss: 1.1880 - accuracy: 0.6928 - val_loss: 0.1340 - val_accuracy: 0.9704\n",
      "\n",
      "Epoch 00001: saving model to best_model.hdf5\n",
      "Epoch 2/3\n",
      "6226/6226 [==============================] - 3252s 522ms/step - loss: 0.7623 - accuracy: 0.8185 - val_loss: 0.1205 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00002: saving model to best_model.hdf5\n",
      "Epoch 3/3\n",
      "6226/6226 [==============================] - 3242s 521ms/step - loss: 0.6799 - accuracy: 0.8488 - val_loss: 0.1119 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00003: saving model to best_model.hdf5\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])\n",
    "\n",
    "model.load_weights('best_model.hdf5') # Подгружаем ранее обученные веса\n",
    "\n",
    "callbacks_list = [checkpoint, earlystop, reduce_lr]\n",
    "\n",
    "# Обучаем\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "    validation_data = test_generator, \n",
    "    validation_steps = test_generator.samples//test_generator.batch_size,\n",
    "    epochs = EPOCHS,\n",
    "    callbacks = callbacks_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../working/model_step4.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1555/1555 [==============================] - 147s 95ms/step - loss: 0.1102 - accuracy: 0.9762\n",
      "Accuracy: 97.62%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6675 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ImageDataAugmentor/image_data_augmentor.py:455: UserWarning: Passing `seed` in `.flow_from_datagrame` has been been removed: pass  `seed` as parameter in `ImageDataAugmentor(..., seed=...)` instead\n",
      "  warnings.warn('Passing `seed` in `.flow_from_datagrame` has been been removed: pass  `seed` '\n"
     ]
    }
   ],
   "source": [
    "test_sub_generator = test_datagen.flow_from_dataframe( \n",
    "    dataframe=sample_submission,\n",
    "    directory=PATH+'test_upload/',\n",
    "    x_col=\"Id\",\n",
    "    y_col=None,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    seed=RANDOM_SEED,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3338/3338 [==============================] - 308s 91ms/step\n"
     ]
    }
   ],
   "source": [
    "test_sub_generator.reset()\n",
    "predictions = model.predict_generator(test_sub_generator, steps=len(test_sub_generator), verbose=1) \n",
    "predictions = np.argmax(predictions, axis=-1) #multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
    "predictions = [label_map[k] for k in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save submit\n"
     ]
    }
   ],
   "source": [
    "filenames_with_dir=test_sub_generator.filenames\n",
    "submission = pd.DataFrame({'Id':filenames_with_dir, 'Category':predictions}, columns=['Id', 'Category'])\n",
    "submission['Id'] = submission['Id'].replace('test_upload/','')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Save submit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>305108.jpg</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295075.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31197.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93598.jpg</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87877.jpg</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id Category\n",
       "0  305108.jpg        7\n",
       "1  295075.jpg        6\n",
       "2   31197.jpg        4\n",
       "3   93598.jpg        7\n",
       "4   87877.jpg        9"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Clean PATH\n",
    "import shutil\n",
    "shutil.rmtree(PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
